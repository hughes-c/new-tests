[CPU]
clock: 2660MHz
num_cores: 1
application: ariel
max_reqs_cycle: 3

[ariel]
executable: ./vectorAdd
gpu_enabled: 1

[Memory]
clock: 200MHz
network_bw: 96GB/s
capacity: 16384MiB

[Network]
#2.66 GHz time period plus slack for ringstop latency
latency: 300ps
# 2.66 GHz clock, moves 64B / cycle, plus overhead -> 36B/c
bandwidth: 96GB/s
flit_size: 8B

#For V100, we have 4 HBM stack, each with 8 channels, so total we have 32 memory parts -- 3 stack x 8chan = 24 mem for titanV
[GPU]
clock: 1200MHz
gpu_cores: 80
gpu_l2_parts: 24
gpu_l2_capacity: 192KiB
#we assume PCIE with 16GB/Sec. Each cycle, we transfer a 4KB page, thus latency = 4/(16*1024*1024) = 23840ps
gpu_cpu_latency: 23840ps
#this is not used for now
gpu_cpu_bandwidth: 16GB/s

[GPUMemory]
#850MHz clock at 1.7Gbps --> 652.8GB/s
clock: 850MHz
#network_bw per mem_part. this should be equal to the actual mem_bw. So, total mem BW = hbmStacks*network_bw = 652 GB/S (in Volta TITANV)
network_bw: 217.6GB/s
#this is the total capacity of all gpu_mem_parts. capacity should be in MiB.
#TitanV has 12GB
capacity: 12288MiB
memControllers: 2
hbmStacks: 3
hbmChan: 4
hbmRows: 16384

[GPUNetwork]
#1200MHz time period plus slack for xbar latency
latency: 750ps
#total BW
bandwidth: 4800GB/s
#BW per xbar link
linkbandwidth: 37.5GB/s
flit_size: 40B
